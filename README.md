# Titanic-survivor
A kaggle's data science challenge 

In this project, I put two machine learning models to the test: Random Forest and XGBoost. They were evaluated under different conditions, such as imbalanced and balanced classes, and with various hyperparameter configurations, to compare their effectiveness.

The results from each stage were submitted to Kaggle and are thoroughly documented in the notebook cells, allowing for a clear analysis of each model's performance.

Neste projeto, eu avaliei dois modelos de machine learning, Random Forest e XGBoost, em diferentes condições para comparar sua eficácia. A análise incluiu cenários com classes desbalanceadas e balanceadas, além de alterações nos hiperparâmetros.

Os resultados de cada etapa foram enviados ao Kaggle e estão detalhadamente documentados nas células do notebook, permitindo uma análise clara do desempenho de cada modelo.
